# 大模型调优调研

## Ref

- https://www.53ai.com/news/qianyanjishu/2024062056319.html

## RAG

### Description

RAG=检索增强生成，主要包含下面三个主要流程：

1. R：检索，从数据库中检索需要的知识条目
2. A：增强，将检索到的知识条目融合到prompt里增强prompt的效果
3. G：生成，大模型生成效果更好的输出，并减缓大模型的幻觉问题

### RAG的目的

RAG主要用于解决下面的问题：

- 幻觉问题：无依据的捏造信息
- 知识库有限：LLM缺少小众知识（e.g. 私有的文件内容）/即时性强的知识（e.g. 最新的新闻和知识）
- 大规模分散知识整合：LLM基于参数限制难以整合大量分散知识，RAG可以通过精确的模糊搜索准确得到这些知识

https://github.com/lizhe2004/Awesome-LLM-RAG-Application?tab=readme-ov-file#sql增强

https://baoyu.io/translations/rag/advanced-rag-techniques-an-illustrated-overview

### 基本的RAG方法和流程

#### RAG的构建

1. 搜集资料并进行清洗和分块
2. 为分块生成向量并聚合为索引，生成键值对
3. 引导大模型执行根据搜索结果参与的上下文回答用户的问题

#### RAG的查询

![](https://baoyu.io/images/rag/advanced-rag-techniques-an-illustrated-overview/0_Ko_ihY8ecAukf2g1.webp)

1. LLM生成并输入查询
2. 对查询使用相同的encoder进行向量化
3. 在数据库中搜索前$k$个相关的键值对
4. 按照结构并入context中
5. LLM生成补全

### 整合RAG的prompt示例

```python
def question_answering(context, query):

    prompt = f"""

Give the answer to the user query delimited by triple backticks ```{query}```\

using the information given in context delimited by triple backticks ```{context}```.\

If there is no relevant information in the provided context, try to answer yourself,

but tell user that you did not have any relevant context to base your answer on.

Be concise and output the answer of size less than 80 tokens.

"""



    response = get_completion(instruction, prompt, model="gpt-3.5-turbo")

    answer = response.choices[0].message["content"]

    return answer
```

### 复杂的RAG技术

![](https://baoyu.io/images/rag/advanced-rag-techniques-an-illustrated-overview/0_Gr_JqzdpHu7enWG9.webp)

#### Chunking & Vectorization

为文档内容创建一个向量索引，查询时计算向量和查询向量之间的最小余弦距离，以查找最近的语义向量。块的大小通常由嵌入模型决定，例如BERT的transformer通常能处理512个token。**通常而言，一个chunk应当表示一个意思，传达一种信息。**例如，一个关于“量子计算”的文档可能被分割为几个部分：

- 段落 1：“量子计算的基本原理是基于量子叠加和量子纠缠现象……”
- 段落 2：“在量子计算中，量子比特可以同时表示0和1……”
- 段落 3：“量子计算的应用包括加密破解和量子模拟……”

权衡：需要提供足够的信息，但也需要文本嵌入足够具体，能有效进行精确搜索

向量化即使用选定的嵌入模型为分块进行稠密向量化表示，用于下一步的检索。常见的搜索专门优化的模型：[bge-large](https://huggingface.co/BAAI/bge-large-en-v1.5) 或 [E5](https://huggingface.co/intfloat/multilingual-e5-large) 嵌入模型

#### 搜索索引

将向量化的语料存储在向量数据库中准备搜索。为加快检索效率，需要设置indexing。基本方法是设置平面索引，即计算查询向量和数据库中向量的几何距离以确定相似程度。

然而，这种全量搜索方法效率不高，因此需要更高效的索引方法。向量索引（近似最近邻居算法）在这里引入，这种方法是一种结构化的数据存储方式，即可以通过某些方式缩小查找范围，而不需要遍历整个数据库。

##### 改进1：ANN算法

**近似最近邻（Approximate Nearest Neighbor, ANN）算法**在效率和精度之间权衡得到平衡，与精确最近邻算法相比，ANN可以在短时间内找到足够接近查询向量的向量，以加快查询速度。这类算法通常使用了聚类、树等类型算法。

常见的向量检索库（数据库）有Faiss、NMSLIB、Annoy等。

LlamaIndex支持多种向量存储索引，还兼容其他简单的索引类型，如列表索引、树索引和关键词表索引，可以用于多种方式的复合距离计算（论文中提到的方法，即复合计算简单索引和语义索引）

> [!tip]
>
> 是否可以根据代码的特点，使用针对性的方法优化RAG的性能，并将其用于修复pattern的提供

##### 改进2:分层索引

索引的结构设计方面，可以基于树的结构进行多层索引，即上层索引集合是下层索引的摘要。如下：

![](https://baoyu.io/images/rag/advanced-rag-techniques-an-illustrated-overview/0_nDwj0Jgpyk2qc_qJ.webp)

这样的搜索分为两步：首先利用摘要来筛选出相关文档，然后只在这个筛选出的相关文档集中继续深入搜索。

##### 改进3:假设性问题

为更好总结分块语料提供的信息，首先使用LLM总结文段为一个假设性问题，表示该文段回答了一个什么样的问题，随后将其向量化，而不是对整个文段向量化。

例如：针对文档段落“量子计算利用量子叠加性”，模型生成的问题可能是“量子计算的叠加性如何工作？”，随后对该问题进行向量化

这样做的好处是可以更好捕捉文段的关键问题，方便进行更精确的检索。

##### 改进4:HyDE

在假设性的问题的基础上增加大模型阅读该文段得出的**该问题的答案**，将问题-答案对进行向量索引化，从而在索引中提供更多信息

##### 改进5:语境增强

##### 改进6: 混合检索

即论文中使用的复合编码器

#### 结果过滤和排序

#### 查询调整

**查询变换是利用大语言模型作为推理引擎，对用户输入进行调整的一系列技术，目的是提升检索的质量。**



### Repos

https://github.com/WangRongsheng/Awesome-LLM-Resourses?tab=readme-ov-file#RAG

### OpenAI对于RAG的原生支持

## SFT

https://cloud.baidu.com/doc/WENXINWORKSHOP/s/9liblgyh7

## Prompt engineering

## Few-shot

## Iter-prompting & CoT

