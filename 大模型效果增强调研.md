# 	大模型调优调研

## Ref

- https://www.53ai.com/news/qianyanjishu/2024062056319.html

## RAG

### Description

RAG=检索增强生成，主要包含下面三个主要流程：

1. R：检索，从数据库中检索需要的知识条目
2. A：增强，将检索到的知识条目融合到prompt里增强prompt的效果
3. G：生成，大模型生成效果更好的输出，并减缓大模型的幻觉问题

### RAG的目的

RAG主要用于解决下面的问题：

- 幻觉问题：无依据的捏造信息
- 知识库有限：LLM缺少小众知识（e.g. 私有的文件内容）/即时性强的知识（e.g. 最新的新闻和知识）
- 大规模分散知识整合：LLM基于参数限制难以整合大量分散知识，RAG可以通过精确的模糊搜索准确得到这些知识



### 基本的RAG方法和流程

#### RAG的构建

1. 搜集资料并进行清洗和分块
2. 为分块生成向量并聚合为索引，生成键值对
3. 引导大模型执行根据搜索结果参与的上下文回答用户的问题

#### RAG的查询

![](https://baoyu.io/images/rag/advanced-rag-techniques-an-illustrated-overview/0_Ko_ihY8ecAukf2g1.webp)

1. LLM生成并输入查询
2. 对查询使用相同的encoder进行向量化
3. 在数据库中搜索前$k$个相关的键值对
4. 按照结构并入context中
5. LLM生成补全

### 整合RAG的prompt示例

```python
def question_answering(context, query):

    prompt = f"""

Give the answer to the user query delimited by triple backticks ```{query}```\

using the information given in context delimited by triple backticks ```{context}```.\

If there is no relevant information in the provided context, try to answer yourself,

but tell user that you did not have any relevant context to base your answer on.

Be concise and output the answer of size less than 80 tokens.

"""



    response = get_completion(instruction, prompt, model="gpt-3.5-turbo")

    answer = response.choices[0].message["content"]

    return answer
```

### 复杂的RAG技术

![](https://baoyu.io/images/rag/advanced-rag-techniques-an-illustrated-overview/0_Gr_JqzdpHu7enWG9.webp)

#### Chunking & Vectorization

为文档内容创建一个向量索引，查询时计算向量和查询向量之间的最小余弦距离，以查找最近的语义向量。块的大小通常由嵌入模型决定，例如BERT的transformer通常能处理512个token。**通常而言，一个chunk应当表示一个意思，传达一种信息。**例如，一个关于“量子计算”的文档可能被分割为几个部分：

- 段落 1：“量子计算的基本原理是基于量子叠加和量子纠缠现象……”
- 段落 2：“在量子计算中，量子比特可以同时表示0和1……”
- 段落 3：“量子计算的应用包括加密破解和量子模拟……”

权衡：需要提供足够的信息，但也需要文本嵌入足够具体，能有效进行精确搜索

向量化即使用选定的嵌入模型为分块进行稠密向量化表示，用于下一步的检索。常见的搜索专门优化的模型：[bge-large](https://huggingface.co/BAAI/bge-large-en-v1.5) 或 [E5](https://huggingface.co/intfloat/multilingual-e5-large) 嵌入模型

#### 搜索索引

将向量化的语料存储在向量数据库中准备搜索。为加快检索效率，需要设置indexing。基本方法是设置平面索引，即计算查询向量和数据库中向量的几何距离以确定相似程度。

然而，这种全量搜索方法效率不高，因此需要更高效的索引方法。向量索引（近似最近邻居算法）在这里引入，这种方法是一种结构化的数据存储方式，即可以通过某些方式缩小查找范围，而不需要遍历整个数据库。

##### 改进1：ANN算法

**近似最近邻（Approximate Nearest Neighbor, ANN）算法**在效率和精度之间权衡得到平衡，与精确最近邻算法相比，ANN可以在短时间内找到足够接近查询向量的向量，以加快查询速度。这类算法通常使用了聚类、树等类型算法。

常见的向量检索库（数据库）有Faiss、NMSLIB、Annoy等。

LlamaIndex支持多种向量存储索引，还兼容其他简单的索引类型，如列表索引、树索引和关键词表索引，可以用于多种方式的复合距离计算（论文中提到的方法，即复合计算简单索引和语义索引）

> [!tip]
>
> 是否可以根据代码的特点，使用针对性的方法优化RAG的性能，并将其用于修复pattern的提供

##### 改进2:分层索引

索引的结构设计方面，可以基于树的结构进行多层索引，即上层索引集合是下层索引的摘要。如下：

![](https://baoyu.io/images/rag/advanced-rag-techniques-an-illustrated-overview/0_nDwj0Jgpyk2qc_qJ.webp)

这样的搜索分为两步：首先利用摘要来筛选出相关文档，然后只在这个筛选出的相关文档集中继续深入搜索。

##### 改进3:假设性问题

为更好总结分块语料提供的信息，首先使用LLM总结文段为一个假设性问题，表示该文段回答了一个什么样的问题，随后将其向量化，而不是对整个文段向量化。

例如：针对文档段落“量子计算利用量子叠加性”，模型生成的问题可能是“量子计算的叠加性如何工作？”，随后对该问题进行向量化

这样做的好处是可以更好捕捉文段的关键问题，方便进行更精确的检索。

##### 改进4:HyDE

在假设性的问题的基础上增加大模型阅读该文段得出的**该问题的答案**，将问题-答案对进行向量索引化，从而在索引中提供更多信息

##### 改进5:语境增强

通过更小的分块获得更高的检索质量，并且在检索到后扩充检索到的内容（上下文）之后再并入prompt

- 句子窗口检索法：每个句子作为一个chunk单独编码，检索到后向上/向下各扩充k个句子后送入LLM
- 自动合并检索法：文档分为更小的child chunk，每个child chunk和parent chunk相关联。**如果在最初检索到的前 *k* 个数据块中有超过 *n* 个数据块与同一个父节点（即更大的数据块）相连，那么我们会用这个父节点来替换提供给大语言模型的上下文。**注意检索只针对子节点。这种方法实现在LlamaIndex的 [递归检索器 + 节点引用](https://docs.llamaindex.ai/en/stable/examples/retrievers/recursive_retriever_nodes.html)。

##### 改进6: 混合检索

> [!note] 
>
> 即论文中使用的复合编码器 这种方式通过简单匹配和语义搜索双向增强代码语料的匹配度

结合基于关键词的搜索（简单匹配）和语义向量搜索（语义搜索）。

结合方法：互惠排名搜索

LangChain实现：Ensemble retriever

#### 结果过滤和排序

得到查询的k最相似结果后对结果进行调整和重排序-称为后处理器

基于相似度分数、关键词、元数据进行过滤，或者使用LLM、Cohere的[Rerank](https://cohere.com/blog/rerank)接口实现重排

![](https://cohere.com/_next/image?url=https%3A%2F%2Fcohere-ai.ghost.io%2Fcontent%2Fimages%2F2023%2F04%2Fdata-src-image-387e0861-93de-4823-84e0-7ae04f2be893.png&w=3840&q=75)

#### 查询调整

**查询变换是利用大语言模型作为推理引擎，对用户输入进行调整的一系列技术，目的是提升检索的质量。**

即通过拆分用户概括的问题为数个详细的小问题，各自获得top-k之后由LLM整合返回最终的答案

![](https://baoyu.io/images/rag/advanced-rag-techniques-an-illustrated-overview/0_DP6RrSA2OkcHnWIV.webp)

实现：分别在 Langchain 中以[多查询检索器](https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever?ref=blog.langchain.dev)的形式和在 Llamaindex 中以[子问题查询引擎](https://docs.llamaindex.ai/en/stable/examples/query_engine/sub_question_query_engine.html)的形式实现

#### 查询路由

**查询路由是指在接收到用户的查询后，由大语言模型决定接下来的操作步骤**，例如直接生成，查询后生成等。一般通过指定LLM生成预定义的指定格式。

### Repos

https://github.com/WangRongsheng/Awesome-LLM-Resourses?tab=readme-ov-file#RAG

- [LangChain](https://github.com/langchain-ai/langchain/) 构建大模型驱动应用程序的框架，提供多模型链式调用、RAG、记忆、工具、prompt设计和管理等功能。:star2:
- [langchain4j](https://github.com/langchain4j/langchain4j) Java版本的langchain
- [LlamaIndex](https://www.llamaindex.ai)用于构建、管理和查询大语言模型（LLM）的索引。它提供了一套工具和框架，允许开发者从非结构化或半结构化数据源中构建信息层，然后对这些数据进行高效查询。LlamaIndex 的主要目标是将LLM与各种数据源集成，从而使模型能够更加智能地处理大规模数据，并能从中提取相关信息。 支持索引构建、查询、数据集成管理、增量索引更新、文档摘要等。:star2:
- [QAnything](https://github.com/netease-youdao/QAnything) youdao开发，支持任意格式的RAG问答框架，支持无文件聊天模式，仅检索模式，自定义Bot模式 (可Docker部署) :star2:
- [Verba](https://github.com/weaviate/Verba) 采用Weaviate的上下文感知数据库的本地RAG，支持多种LLM、嵌入模型、分块模型等，提供友好的前端界面（可docker部署）:star2:
- [Dify](https://github.com/langgenius/dify)基于Devops的应用程序开发导向开源LLM开发平台，**支持Agent，工具，pipeline，RAG** :desktop_computer:
- [Danswer](https://github.com/danswer-ai/danswer)可以整合到slack的RAG助手，可以手动选择进行chat的文档（可Docker部署）
- [RAGFlow](https://github.com/infiniflow/ragflow) 基于OCR的文档RAG，支持文档结构、图片、表格的深度解析
- [Cognita](https://github.com/truefoundry/cognita) 模块化、可配置、API驱动的RAG。**Cognita 默认还支持增量索引**（这个对迭代中的代码可能有很大作用）:star2:
- [LazyLLM](https://github.com/LazyAGI/LazyLLM) 一款低代码构建**多Agent大模型**应用的开发工具，协助开发者用极低的成本构建复杂的AI应用，并可以持续的迭代优化效果。且处于活跃开发中(**支持多agent+RAG+tools**) :desktop_computer:
- [GraphRAG](https://github.com/microsoft/GraphRAG) 微软出品的基于图的RAG，从非结构化的文本中提取结构化的数据，以构建知识图谱https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/。这种方法通常在基于整个文档上进行阅读并总结的使用场景下存在优势。

> [!note]
>
> 可以先期针对一个知名且结构相对简单的开源项目 尝试进行检测 首先基于已知漏洞进行检测并以已知的检测率作为改进指标 因为RAG对于能力的提升是以项目为单位的，在大项目下可能有更好的效果
>
> 其次针对开源项目代码更新频繁的情况 可以做一个增量RAG看看 可以先调查一下

### OpenAI对于RAG的原生支持

OpenAI系列的模型从4开始提供对RAG的原生支持。OpenAI提供了两个嵌入模型，text-embedding-3-small and text-embedding-3-large，用于将语料分块并向量化，返回嵌入向量，用户可以自行处理。同时在一般的chatbot场景下，可以通过上传资料构建一个RAG，以在chat中自动化的检索和生成。

> [!Note]
>
> 这一段需要更多佐证 准确性存疑

### Refs

https://github.com/lizhe2004/Awesome-LLM-RAG-Application?tab=readme-ov-file#sql增强

https://baoyu.io/translations/rag/advanced-rag-techniques-an-illustrated-overview

## SFT

https://cloud.baidu.com/doc/WENXINWORKSHOP/s/9liblgyh7

**监督微调**（Supervised Fine-Tuning, SFT），即在大模型的基础上通过提供更多专业细分的训练数据（已标注）对LLM进行进一步训练的方法，以进一步强化LLM在某一个领域的能力。SFT需要调整模型的参数，本质上是一种微调。

具体来说，SFT调优的主要过程如下：

**基础模型**：首先，从一个已经训练好的大模型（例如GPT等预训练模型）开始。这个基础模型通常在大规模无监督数据上训练，学习了广泛的语言理解能力。

**监督数据**：为调优准备**有监督的任务数据**，这些数据通常是成对的输入和期望的输出。对于语言模型，输入可以是问题或命令，输出则是期望模型生成的答案或回应。数据的质量对SFT调优的效果至关重要。

**微调过程**：通过有监督的数据，模型继续进行训练。在这个过程中，模型会学习根据输入生成更符合要求的输出，而不仅仅是基于原始预训练时的通用能力。

**优化目标**：在微调过程中，使用损失函数（如交叉熵损失）来衡量模型输出与期望输出的差距，并**通过反向传播不断优化模型的参数，使其生成的结果更加准确。**

> RePair: Automated Program Repair with Process-based Feedback

> [!Note]
>
> 除了SFT外，还有数种调整参数的微调方法

## Few-shot

Few-shot调优是一种在机器学习和自然语言处理（NLP）中的技术，旨在通过少量的训练数据或样本（称为支持集）（通常是几例到几十例）来调整预训练模型，使其能够适应特定任务。这种技术依赖于模型的预训练能力，即模型在大规模数据上已经接受了大量的训练，掌握了广泛的知识，调优阶段则仅需少量的新数据来适应新的任务。

few阶段主要是通过少量的例子得到任务的具体要求，而不是学习知识。因此对于支持集的要求较高。Few-shot调优的目标是让模型能够快速适应新的任务，因此通常会结合任务特定的提示词或上下文信息，来引导模型理解新任务的目标。

### 和SFT的不同点

**Few-shot调优**和**SFT（Supervised Fine-Tuning，监督微调）**都是在预训练模型基础上进行微调的方法，但它们的核心区别在于**样本数量、任务适应性**和**微调的方式**。下面我会详细解释两者的概念和关键区别：

1. **Few-shot 调优**

Few-shot调优指的是在预训练模型上使用**非常少量的数据**（通常是几例到几十例）进行微调，使模型适应特定任务。它的核心思想是充分利用预训练模型的已有知识，通过少量的样本让模型学会处理新任务。

**Few-shot 调优的特点：**

- **样本数量极少**：通常只有几例到几十例。
- **依赖预训练模型的能力**：Few-shot 调优依赖于预训练模型已经具备丰富的上下文理解能力，少量的数据用于调整模型使其适应新任务。
- **不需要大规模数据**：Few-shot 适用于数据非常稀缺或标注困难的场景，目的是通过极少量的数据让模型学会执行任务。
- **任务适应性强**：Few-shot 可能结合提示词（prompts）或者上下文，直接引导模型完成任务。

**典型应用**：使用GPT、BERT等大型语言模型进行文本分类、情感分析等任务时，如果只有少量的标注数据，Few-shot 调优就可以发挥作用。

2. **SFT（Supervised Fine-Tuning）监督微调**

SFT是指使用**大量标注数据**对预训练模型进行监督学习微调。在这种方法中，模型通过大量的训练数据调整其权重，以适应具体任务。这种方式广泛应用于模型从预训练到专门任务的过渡阶段。

**SFT 的特点：**

- **样本数量较大**：SFT通常需要相对大量的标注数据来实现有效的模型微调。
- **适用特定任务**：在SFT过程中，模型通过大量的标注数据逐渐调整其参数，以适应具体的任务。任务可以是文本分类、机器翻译、问答系统等。
- **对大数据的依赖**：相比Few-shot，SFT依赖大规模的高质量标注数据来实现高效的任务微调。
- **模型更新较大**：通过监督微调，模型参数会进行大幅度更新，直到模型能够准确完成该特定任务。

**典型应用**：对于机器翻译、文本生成、图像分类等任务，SFT可以在预训练模型的基础上，通过大量的标注数据进行微调，确保模型在特定任务上表现良好。

**Few-shot调优 和 SFT 的关键区别**

| 特性               | Few-shot 调优                        | SFT（监督微调）                  |
| ------------------ | ------------------------------------ | -------------------------------- |
| **样本数量**       | 极少量数据（几例到几十例）           | 大量数据（通常需要数百到数千例） |
| **依赖预训练模型** | 高度依赖预训练模型已有的能力         | 通过大量数据微调模型参数         |
| **任务适应性**     | 快速适应少量样本，结合提示词或上下文 | 需要较长的训练时间和大量数据适应 |
| **适用场景**       | 数据稀缺或昂贵的场景                 | 数据充足且需要高精度任务         |
| **模型参数调整**   | 参数调整较少，仅微调                 | 大规模更新模型参数               |

举个例子来说明：

- **Few-shot 调优**：
    假设你有一个预训练的语言模型，比如 GPT-3，你希望它执行情感分类任务，但手上只有 10 条标注的情感数据。这时，你可以使用这 10 条数据通过 Few-shot 调优快速调整模型，使其适应情感分类任务，而不需要大量的额外数据。

- **SFT（监督微调）**：
    假设你有一个预训练的模型，你想让它执行问答任务。你拥有一个包含数千对问题和答案的数据集。在这种情况下，你可以通过SFT使用这个大规模的标注数据集来对模型进行充分的微调，让模型更精确地回答问题。

总结

- **Few-shot调优**适用于样本数据稀缺的情况，模型通过少量样本和已有的预训练知识快速适应新任务。
- **SFT（监督微调）**则是在预训练的基础上，依赖大量标注数据对模型进行大规模的调整和训练，以适应某个特定任务。

两者的关键区别在于**数据量**和**模型的微调方式**。Few-shot 调优更加灵活和高效，适合快速适应新任务；而SFT依赖于大规模数据，通常用于高精度的任务定制化。

## Iter-prompting & CoT

CoT（Chain of Thought）是一种优化大语言模型（LLM）推理能力的方法，通过引导模型逐步推理，从而提高其复杂问题的解答效果。相比于直接输出答案，CoT方法模拟了人类思考时逐步分解问题的过程，有助于模型更好地理解问题的逻辑层次，特别是在处理数学、推理和常识推理等复杂任务时。

